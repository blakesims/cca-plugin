name: Voice-to-Text Transcriber
description: >
  Build your own local voice-to-text app. A floating pill on your desktop that
  records speech, transcribes with AI, and pastes the text wherever you need it.
  Runs locally — no API keys, no subscriptions, your data stays private.
version: "0.2"

# Progressive customisation: each level includes all questions from levels below it.
# The PRD skill asks "How deep do you want to configure?" first, then asks
# only the questions for that level + all lower levels.
# Keys not asked get their value from `defaults`.

defaults:
  app_name: my-transcriber
  use_case: general
  model: base.en
  language: en
  output: clipboard
  autopaste: true
  streaming: false
  hotkey: true
  vad: false

levels:
  - name: Quick Start
    description: Sensible defaults, start building in minutes
    questions:
      - key: app_name
        question: "What do you want to call your app?"
        example: "my-transcriber"
      - key: use_case
        question: "What will you mainly use it for?"
        options:
          meetings: "Transcribe meetings and calls"
          journaling: "Voice journaling / daily notes"
          coding: "Dictate code comments and docs"
          general: "General voice-to-text for anything"

  - name: Customise
    description: Pick your model, output style, and language
    questions:
      - key: model
        question: "Speed or accuracy?"
        options:
          tiny.en: "Fastest, decent accuracy (~40MB download)"
          base.en: "Balanced — recommended (~75MB download)"
          small.en: "Most accurate, needs more RAM (~250MB download)"
        default: base.en
      - key: output
        question: "Where should transcribed text go?"
        options:
          clipboard: "Auto-copy to clipboard"
          file: "Save to a text file"
        default: clipboard
      - key: autopaste
        question: "Auto-paste after transcription?"
        options:
          false: "No — just copy to clipboard, I'll paste myself"
          true: "Yes — automatically paste into the active window (like magic)"
        default: true
      - key: language
        question: "English only or multiple languages?"
        options:
          en: "English only (faster, more accurate)"
          multi: "Multiple languages (auto-detect)"
        default: en

  - name: Architect
    description: Advanced features — streaming, hotkeys, voice detection
    questions:
      - key: streaming
        question: "Record-and-transcribe or live streaming?"
        options:
          false: "Record → stop → transcribe (simpler, recommended)"
          true: "Live transcription as you speak (advanced)"
        default: false
      - key: hotkey
        question: "Global hotkey to start/stop recording?"
        options:
          false: "No hotkey — click the pill to record"
          true: "Yes — global keyboard shortcut (recommended)"
        default: true
      - key: vad
        question: "Voice activity detection (auto-stop on silence)?"
        options:
          false: "No — manual start/stop"
          true: "Yes — auto-detect when you stop talking"
        default: false

  - name: Freeform
    description: Blank canvas — describe exactly what you want and I'll write the PRD
    type: conversation

# WSL: this kit requires audio input, GUI, and OS-level integration.
# None of these work reliably in WSL. If the student chose to proceed
# from WSL, all build agents MUST follow these rules:
wsl:
  requires_native_os: true
  reason: "Audio capture, PySide6 GUI, and global hotkeys do not work in WSL"
  build_rules:
    - "Put the project in /mnt/c/Users/<username>/projects/<app_name>/ so Windows can access it"
    - "Write all code normally in WSL — the code itself is platform-independent"
    - "Do NOT run the app from WSL. Do NOT try python main.py or test_transcribe.py for audio/GUI tests"
    - "Do NOT debug audio device errors or display server errors — they are WSL limitations, not code bugs"
    - "Unit tests that mock audio (e.g. transcription from a WAV file) CAN run in WSL"
    - "For every phase that needs real testing, give the student simple PowerShell instructions"
  test_instructions: |
    After each phase, tell the student exactly how to test from Windows.
    Use this template — adapt for the specific phase:

    > **Time to test!** Open PowerShell and run these commands:
    >
    > ```powershell
    > cd C:\Users\<username>\projects\<app_name>
    > python -m venv .venv
    > .venv\Scripts\activate
    > pip install -r requirements.txt
    > python test_transcribe.py
    > ```
    >
    > (You only need to create the venv and install requirements the first time.
    > After that, just activate and run.)
    >
    > Tell me what happened and I'll help you fix any issues.

    The venv + pip install only needs to happen once. After that, just:
    > ```powershell
    > cd C:\Users\<username>\projects\<app_name>
    > .venv\Scripts\activate
    > python main.py
    > ```

    IMPORTANT: The student is a beginner. Keep the PowerShell steps minimal
    and explain what each command does the first time.

# Build scope: defines how the PRD should be decomposed into tasks.
# Each scope group becomes a separate task with its own plan/execute/review cycle.
#
# For this kit, everyone builds a desktop app. The core engine is built first
# as importable modules (not a CLI app — students don't need terminal skills).
# Then the desktop UI wraps it. System integration adds hotkey/paste/VAD on top.

build_scope:
  - name: Recording & Transcription Engine
    slug: core
    description: >
      The engine that powers everything: record audio, transcribe with AI, output text.
      Built as Python modules that the desktop app will import. Includes a small test
      script to verify it works (not a full CLI app).
    always_included: true
    features:
      - Project scaffolding (venv, requirements.txt, config.yaml)
      - recording.py — microphone capture via sounddevice (16kHz mono), numpy buffer, WAV temp file
      - transcription.py — faster-whisper model loading and transcription (model auto-downloads)
      - output.py — clipboard copy (pyperclip), file save, based on config
      - config.py — load config.yaml with sensible defaults
      - test_transcribe.py — simple script that records 5 seconds and prints the result
    success_criteria:
      - "Run test_transcribe.py, speak for 5 seconds, see accurate text printed"
      - "Model downloads automatically on first run"
      - "Works offline after first model download"
    agent_notes: >
      Student learning project. Build clean, importable Python modules — NOT a CLI app
      or interactive terminal program. The test script is just for verification.
      Do NOT plan for any UI, hotkeys, or system integration — those are separate tasks.
      Do NOT worry about thread safety yet — the UI task will handle that.
      Keep it simple: record → save WAV → transcribe → return text.
      ON WSL: Do NOT run test_transcribe.py (no audio device). Instead write a
      test_from_file.py that transcribes a bundled sample.wav to verify the engine works.
      Give the student PowerShell instructions to test real microphone recording (see wsl.test_instructions).

  - name: Desktop App
    slug: desktop-app
    description: >
      A tiny floating pill that sits on top of all your windows. Click to record,
      click to stop, see your words appear. Wraps the recording engine in a
      professional-looking PySide6 interface.
    always_included: true
    depends_on: [core]
    features:
      - main.py — app entry point, PySide6 QApplication setup
      - pill_widget.py — frameless always-on-top floating pill window
      - State machine (IDLE → RECORDING → TRANSCRIBING → IDLE)
      - Waveform visualization (20 bars, RMS log-scaled)
      - Size transitions animated (200ms OutCubic easing)
      - Drag-to-reposition with position persistence
      - Right-click context menu (model selection, input device, quit)
      - Tokyo Night color scheme
      - Worker threads for recording/transcription (keep UI responsive)
    success_criteria:
      - "Run main.py, see floating pill on screen"
      - "Click pill → recording starts with waveform, click again → transcription appears"
      - "Pill stays on top of other windows without stealing focus"
      - "Transcribed text copied to clipboard"
    agent_notes: >
      Student project. The recording and transcription engine already exists from the
      previous task — import recording.py, transcription.py, output.py. Do NOT rewrite
      the engine. This task ONLY builds the UI.
      Use Qt signals/slots for thread-safe UI updates from worker threads.
      Window flags: Qt.SplashScreen | Qt.FramelessWindowHint | Qt.WindowStaysOnTopHint
      Tokyo Night: bg #1a1b26 94% opacity, accent #7aa2f7, recording dot #7dcfff, text #c0caf5
      ON WSL: Do NOT run main.py (no display server). Write the code, then give the
      student PowerShell instructions to test the GUI (see wsl.test_instructions).

  - name: System Integration
    slug: system-integration
    description: >
      Make it feel native: global hotkey to start/stop from anywhere, auto-paste
      text into whatever app you're using, and voice detection to auto-stop recording.
    condition: "any(hotkey, autopaste, vad)"
    depends_on: [core, desktop-app]
    features:
      - Global hotkey toggle via pynput (if hotkey enabled)
      - Auto-paste into active window via Cmd+V/Ctrl+V simulation (if autopaste enabled)
      - Active window tracking per platform (if autopaste enabled)
      - Voice activity detection for auto-stop (if vad enabled)
      - macOS permission handling (Accessibility for hotkey/paste, Microphone for recording)
    success_criteria:
      - "Press hotkey → recording starts, press again → stops and transcribes"
      - "Transcribed text appears in the app you were just typing in"
    agent_notes: >
      Student project. The desktop app already works from previous tasks.
      This task adds OS-level integrations ON TOP of the existing app.
      Only implement features the student actually enabled (check config).
      Handle macOS Accessibility permission request gracefully.
      Do NOT modify the core engine or UI — just add integration layers.
      ON WSL: Hotkeys and auto-paste must target Windows, not Linux. Use
      Windows-native pynput behaviour. Give student PowerShell test instructions
      (see wsl.test_instructions).

prd_template: |
  # {{app_name}} — Voice-to-Text Transcriber

  ## Problem Statement
  I need a local voice-to-text tool for {{use_case}}. It should run entirely on my machine
  with no cloud APIs, keeping my audio private. It lives as a tiny floating pill on my
  desktop — always there when I need it.

  ## Must Have (v1)
  1. Floating pill window that stays on top of all apps without stealing focus
  2. Click to record → click to stop → see transcription
  3. Transcribe speech using local AI model ({{model}}) — no internet needed after setup
  4. {{#output_clipboard}}Auto-copy transcribed text to clipboard{{/output_clipboard}}{{#output_file}}Save transcriptions to a text file{{/output_file}}
  {{#autopaste}}5. Auto-paste transcribed text into the active window after transcription{{/autopaste}}
  {{#hotkey}}6. Global hotkey to start/stop recording from anywhere{{/hotkey}}
  7. Config file (`config.yaml`) to change model, language, and output without editing code

  ## Technical Approach

  **Stack:**
  - Python 3.10+
  - `faster-whisper` — local Whisper model inference (pip install, no compilation needed)
  - `sounddevice` — microphone capture (ships its own PortAudio drivers, no system deps)
  - `numpy` — audio buffer handling
  - `scipy` — WAV file writing
  - `PySide6` — floating always-on-top window (Qt framework, professional look)
  {{#output_clipboard}}- `pyperclip` — cross-platform clipboard access{{/output_clipboard}}
  {{#hotkey}}- `pynput` — global keyboard shortcut listener{{/hotkey}}
  {{#autopaste}}- `pynput` — simulate Cmd+V / Ctrl+V to auto-paste{{/autopaste}}

  **Architecture:**
  ```
  Microphone → sounddevice (capture) → numpy buffer → WAV temp file → faster-whisper ({{model}}) → text → output
  ```

  **Recording flow:**
  {{^streaming}}1. User clicks the pill (or presses hotkey)
  2. Pill expands, shows waveform — audio streams into a buffer via sounddevice callback
  3. User clicks again (or presses hotkey) to stop
  4. Pill shows "transcribing..." spinner
  5. Buffer saved to temp WAV file → faster-whisper transcribes
  6. Text copied to clipboard {{#autopaste}}and auto-pasted into active window{{/autopaste}}
  7. Pill returns to idle state{{/streaming}}
  {{#streaming}}1. User clicks the pill (or presses hotkey)
  2. Audio streams continuously via sounddevice callback
  3. Chunks sent to faster-whisper in real-time
  4. Partial transcriptions displayed as they arrive
  5. User clicks again to stop — final text assembled{{/streaming}}

  **Floating pill UI — state machine:**

  | State | Size | Contents | Behaviour |
  |-------|------|----------|-----------|
  | IDLE | 36×20px | Tiny rounded pill, solid dark background | Draggable, click to start recording |
  | RECORDING | 130-250×28px | Pulsing dot + mini waveform (20 bars, log-scaled) | Expands with animation, click to stop |
  | TRANSCRIBING | 60-100×24px | Spinner + progress % label | Collapses slightly, auto-returns to IDLE |

  **Window flags (PySide6):**
  ```python
  Qt.SplashScreen | Qt.FramelessWindowHint | Qt.WindowStaysOnTopHint | Qt.WindowDoesNotAcceptFocus
  ```
  Plus: `Qt.WA_TranslucentBackground`, `Qt.WA_ShowWithoutActivating`

  **Color scheme (Tokyo Night):**
  - Background: `#1a1b26` with 94% opacity
  - Accent (waveform/links): `#7aa2f7` (blue)
  - Recording dot: `#7dcfff` (cyan), pulsing between 40-100% opacity
  - Text: `#c0caf5`
  - Text dim: `#565f89`

  **Key behaviours:**
  - Always on top, never steals focus from the active window
  - Draggable — position persists in settings file across restarts
  - Default position: bottom center of screen, 12px from bottom edge
  - Size transitions animated (200ms, OutCubic easing)
  - Right-click context menu: model selection, input device, quit
  - Waveform uses RMS with log scaling: `db = 20 * log10(rms + 1e-10)`, mapped from -80dB to -20dB

  **Model:** `{{model}}` from faster-whisper (auto-downloads on first run)
  {{#language_en}}English-only model — faster and more accurate for English speech.{{/language_en}}
  {{#language_multi}}Multilingual model with automatic language detection.{{/language_multi}}

  ## Configuration
  `config.yaml` at project root:
  ```yaml
  model: {{model}}
  language: {{language}}
  output: {{output}}
  {{#hotkey}}hotkey: ctrl+f{{/hotkey}}
  ```
  Users can change these without editing Python code.

  ## Platform Notes
  - Works on macOS and Linux out of the box
  - **Windows users: use native Python, NOT WSL** — WSL has no reliable microphone access
  - macOS: terminal/IDE will prompt for microphone permission on first run — just click Allow
  - First run downloads the model (~75MB for base.en) — needs internet once
  {{#hotkey}}- macOS: global hotkeys require Accessibility permission in System Settings{{/hotkey}}
  {{#autopaste}}- macOS: auto-paste requires Accessibility permission in System Settings{{/autopaste}}

  ## Success Criteria
  - Floating pill appears on screen, stays on top without stealing focus
  - Record a 10-second clip and see accurate transcription
  - Model loads in under 5 seconds on modern hardware
  - Transcription completes in under 2 seconds for a 10-second clip
  - Works without internet after first model download
  {{#output_clipboard}}- Transcribed text appears in clipboard ready to paste{{/output_clipboard}}
  {{#autopaste}}- Text auto-pastes into the previously active window{{/autopaste}}

  ## Out of Scope
  - Speaker diarization (who said what)
  - Real-time translation
  - Cloud API fallback
  - Mobile support
  - Post-processing / grammar cleanup (future enhancement)

  ## Install
  ```bash
  pip install faster-whisper sounddevice numpy scipy PySide6 pyperclip{{#hotkey}} pynput{{/hotkey}}{{#autopaste}} pynput{{/autopaste}}
  ```
